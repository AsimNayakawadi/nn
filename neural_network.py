# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JoHVVgLuZqz5yb2IubjFxUqwLqODz0FX
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, roc_curve, auc,recall_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
## For Grid Search
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
## for data preprocessing
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder
# %matplotlib inline

from google.colab import drive
drive.mount("/content/gdrive")

import os 
PATH = "/content/gdrive/MyDrive/google"
os.chdir(PATH)

data = pd.read_csv('Price.csv')

data.head()

data.dtypes

data.nunique()

data = data.drop(['Unnamed: 0','flight'],axis=1)

x = data.drop(["price"],axis=1)
y = data['price']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=101)

x_train

data.dtypes

data.select_dtypes(include="object").columns

col = ['airline', 'source_city', 'departure_time', 'stops',
       'arrival_time', 'destination_city', 'class']

data[col] = data[col].astype('category')

data.dtypes

data.info()

num = ['duration','days_left' ]

data.isnull().sum()

col_imputer = SimpleImputer(strategy='most_frequent')
num_imputer = SimpleImputer(strategy='mean')

x_train_num=x_train.drop(col,axis=1)
x_train_col=x_train.drop(num,axis=1)

x_test_num=x_test.drop(col,axis=1)
x_test_col=x_test.drop(num,axis=1)
x_train.columns

x_train_num=pd.DataFrame(num_imputer.fit_transform(x_train_num),columns=x_train_num.columns)
x_train_col=pd.DataFrame(col_imputer.fit_transform(x_train_col),columns=x_train_col.columns)

x_test_num=pd.DataFrame(num_imputer.transform(x_test_num),columns=x_test_num.columns)
x_test_col=pd.DataFrame(col_imputer.transform(x_test_col),columns=x_test_col.columns)

x_test_col.isna().sum()

# standardization of numeric data
scaler = StandardScaler()

x_train_num=pd.DataFrame(scaler.fit_transform(x_train_num),columns=x_train_num.columns)

x_train_num.shape

x_test_num.shape

#one hot encoding

ohe = OneHotEncoder(handle_unknown='ignore')

x_train_col = pd.DataFrame(ohe.fit_transform(x_train_col).todense(), columns = ohe.get_feature_names_out())

x_train_col.head()

x_test_col = pd.DataFrame(ohe.transform(x_test_col).todense(), columns = ohe.get_feature_names_out())

x_test_col.head()

## Combining Numeric and Categorical Data - use concat

Train = pd.concat([x_train_num,x_train_col],axis=1)

Test = pd.concat([x_test_num, x_test_col],axis=1)

Train.shape

Train.head()

import warnings
warnings.filterwarnings('ignore')

from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier

def create_model(hidnodes):
    model = Sequential()
    model.add(Dense(hidnodes, input_dim=37, activation='tanh'))
    model.add(Dense(1, activation='linear'))
    model.compile(loss='binary_crossentropy',optimizer='sgd',
                  metrics=['mse'])
    return model

batch_size = [64]
hidnodesgrid =[10,20,30,40]
pram_grid = dict(batch_size=batch_size,hidnodes=hidnodesgrid,epochs=[100])

model = KerasClassifier(build_fn = create_model, verbose =0)
grid = GridSearchCV(estimator=model,param_grid=pram_grid,return_train_score=True)

perceptron_model = Sequential()
perceptron_model.add(Dense(40, input_dim=37, activation='relu'))
perceptron_model.add(Dense(16, activation = 'relu'))
perceptron_model.add(Dense(1, activation='linear'))

optimizer = tf.keras.optimizers.SGD(momentum=0.9,learning_rate=0.0001)

perceptron_model.compile(loss='mean_squared_error', 
                         optimizer=optimizer, 
                         metrics=['mae'])

perceptron_model.summary()

perceptron_model_history = perceptron_model.fit(Train, 
                                                y_train, 
                                                epochs=50, 
                                                batch_size=32, 
                                                validation_split=0.2)

# lets write a function to get the metrics & loss plots

def plot(model_history, plot_type):
    val_type = "val_"+plot_type
    plt.plot(model_history.history[plot_type])
    plt.plot(model_history.history[val_type])
    plt.title('MSE Plot')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'])
    plt.show()

perceptron_model

print(perceptron_model_history.history.keys())

plot(perceptron_model_history, 'mae')

plot(perceptron_model_history, 'loss')

